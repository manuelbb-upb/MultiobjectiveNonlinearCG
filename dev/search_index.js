var documenterSearchIndex = {"docs":
[{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"EditURL = \"https://github.com/manuelbb-upb/MultiobjectiveNonlinearCG/blob/main/src/multidir_frank_wolfe.jl\"","category":"page"},{"location":"generated/multidir_frank_wolfe/#Custom-Frank-Wolfe-Solver...","page":"Frank-Wolfe Solver","title":"Custom Frank-Wolfe Solver...","text":"","category":"section"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"... to compute the multi-objective steepest descent direction cheaply. For unconstrained problems, the direction can be computed by projecting symbf0in ℝ^n onto the convex hull of the negative objective gradients. This can be done easily with JuMP and a suitable solver (e.g., COSMO).","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"In “Multi-Task Learning as Multi-Objective Optimization” by Sener & Koltun, the authors employ a variant of the Frank-Wolfe-type algorithms defined in “Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization” by Jaggi.","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"The objective for the projection problem is","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"F(symbfα)\n= frac12  sum_i=1^K αᵢ fᵢ _2^2\n= frac12  nabla symbff^T symbfα _2^2\n= frac12 symbf a^T nabla symbff nabla symbff^T symbf α","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Hence,","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"nabla F(symbfα)\n= nabla symbff nabla symbff^T symbf α\n= symbf M symbf α","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"The algorithm starts with some initial symbfα = α_1  α_K^T and optimizes F within the standard simplex S = symbf α = α_1  α_k α_i ge 0 sum_i α_i = 1 This leads to the following procedure:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Compute seed s as the minimizer of langle symbf s nabla F(symbf α_k) rangle = langle symbf s symbf M symbf α_krangle in S. The minimum is attained in one of the corners, i.e., symbf s = symbf e_t, where t is the minimizing index for the entries of symbf M symbf α_k.\nCompute the exact stepsize γin01 that minimizes\nF((1-γ)symbf α_k + γ symbf s)\nSet symbf α_k+1 = (1-γ_k) α_k + γ_k symbf s.","category":"page"},{"location":"generated/multidir_frank_wolfe/#Finding-the-Stepsize","page":"Frank-Wolfe Solver","title":"Finding the Stepsize","text":"","category":"section"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Let's discuss step 2. Luckily, we can indeed (easily) compute the minimizing stepsize. Suppose v  ℝⁿ and u  ℝⁿ are vectors and M  ℝ^nn is a symmetric square matrix. What is the minimum of","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"σ(γ) = ( (1-γ) v + γ u )ᵀ M ( (1-γ) v + γ u)    (γ  01)","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"We have","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"σ(γ) beginalignedt\n\t=\n\t( (1-γ) v + γ u )ᵀ M ( (1-γ) v + γ u )\n\t\t\n\t=\n\t(1-γ)² underbracevᵀ M v_a + 2γ(1-γ) underbraceuᵀ M v_b + γ² underbraceuᵀ M u_c\n\t\t\n\t=\n\t(1 + γ² - 2γ)a + (2γ - 2γ²)b + γ² c\n\t\t\n\t=\n\t(a -2b + c) γ² + 2 (b-a) γ + a\nendaligned","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"The boundary values are","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"σ₀ = σ(0) = a textand σ₁ = σ(1) = c","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"If (a-2b+c)  0  a-b  b-c, then the parabola is convex and has its global minimum where the derivative is zero:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"2(a - 2b + c) y^* + 2(b-a) stackrel= 0\n \n\tγ^* = frac-2(b-a)2(a -2 b + c)\n\t\t= fraca-b(a-b)+(c-b)","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"If a-b  b -c, the parabola is concave and this is a maximum. The extremal value is","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"σ_* = σ(γ^*)\n\t= frac(a - b)^2(a-b)+(c-b) - frac2(a-b)^2(a-b) + (c-b) + a\n\t= a - frac(a-b)^2(a-b) + (c-b)","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"\"\"\"\n\tmin_quad(a,b,c)\n\nGiven a quadratic function ``(a -2b + c) γ² + 2 (b-a) γ + a`` with ``γ ∈ [0,1]``, return\n`γ_opt` minimizing the function in that interval and its optimal value `σ_opt`.\n\"\"\"\nfunction min_quad(a,b,c)\n\ta_min_b = a-b\n\tb_min_c = b-c\n\tif a_min_b > b_min_c\n\t\t# the function is a convex parabola and has its global minimum at `γ`\n\t\tγ = a_min_b /(a_min_b - b_min_c)\n\t\tif 0 < γ < 1\n\t\t\t# if its in the interval, return it\n\t\t\tσ = a - a_min_b * γ\n\t\t\treturn γ, σ\n\t\tend\n\tend\n\t# the function is either a line or a concave parabola, the minimum is attained at the\n\t# boundaries\n\tif a <= c\n\t\treturn 0, a\n\telse\n\t\treturn 1, c\n\tend\nend","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"To use the above function in the Frank-Wolfe algorithm, we define a helper according to the definitions of ab and c:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"function min_chull2(M, v, u)\n\tMv = M*v\n\ta = v'Mv\n\tb = u'Mv\n\tc = u'M*u\n\treturn min_quad(a,b,c)\nend","category":"page"},{"location":"generated/multidir_frank_wolfe/#Completed-Algorithm","page":"Frank-Wolfe Solver","title":"Completed Algorithm","text":"","category":"section"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"The stepsize computation is the most difficult part. Now, we only have to care about stopping and can complete the solver for our sub-problem:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"import LinearAlgebra as LA","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"function frank_wolfe_multidir_dual(grads; max_iter=10_000, eps_abs=1e-5)\n\n\tnum_objfs = length(grads)\n\tT = Base.promote_type(Float32, mapreduce(eltype, promote_type, grads))\n\n\t# 1) Initialize ``α`` vector. There are smarter ways to do this...\n\tα = fill(T(1/num_objfs), num_objfs)\n\n\t# 2) Build symmetric matrix of gradient-gradient products\n\t# `_M` will be a temporary, upper triangular matrix\n\t_M = zeros(T, num_objfs, num_objfs)\n\tfor (i,gi) = enumerate(grads)\n\t\tfor (j, gj) = enumerate(grads)\n\t\t\tj<i && continue\n\t\t\t_M[i,j] = gi'gj\n\t\tend\n\tend\n\t### mirror `_M` to get the full symmetric matrix\n\tM = LA.Symmetric(_M, :U)\n\n\t# 3) Solver iteration\n\t_α = copy(α)    # to keep track of change\n\tfor _=1:max_iter\n\t\tt = argmin( M*α )\n\t\tv = α\n\t\tu = zeros(T, num_objfs)\n\t\tu[t] = one(T)\n\n\t\tγ, _ = min_chull2(M, v, u)\n\n\t\tα .*= (1-γ)\n\t\tα[t] += γ\n\n\t\tif sum( abs.( _α .- α ) ) <= eps_abs\n\t\t\tbreak\n\t\tend\n\t\t_α .= α\n\tend\n\n\t# return -sum(α .* grads) # somehow, broadcasting leads to type instability here,\n\t# see also https://discourse.julialang.org/t/type-stability-issues-when-broadcasting/92715\n\treturn mapreduce(*, +, α, grads)\nend","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#MultiobjectiveNonlinearCG.jl","page":"MultiobjectiveNonlinearCG.jl","title":"MultiobjectiveNonlinearCG.jl","text":"","category":"section"},{"location":"","page":"MultiobjectiveNonlinearCG.jl","title":"MultiobjectiveNonlinearCG.jl","text":"Documentation for MultiobjectiveNonlinearCG.jl","category":"page"}]
}
