var documenterSearchIndex = {"docs":
[{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"EditURL = \"https://github.com/manuelbb-upb/MultiobjectiveNonlinearCG/blob/main/docs/src/literate_jl/two_rosenbrock.jl\"","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"include(joinpath(joinpath(\"..\", \"literate_jl\"), \"makie_theme.jl\")) #hide\nnothing #hide","category":"page"},{"location":"generated/two_rosenbrock/#Two-Rosenbrock-Functions","page":"2 Rosenbrocks","title":"Two Rosenbrock Functions","text":"","category":"section"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"In single-objective optimization, the Rosenbrock function is a prominent example of an objective function that is hard for vanilla steepest descent. It has a flat valley around its global optimum, so the gradients become small.","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"In this example, we are going to look at a bi-objective problem constructed from parameterized Rosenbrock functions. The Rosenbrock function","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"f_ab(x_1 x_2) = b( x_2 - x_1^2 )^2 + (a - x_1)^2","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"has its global minimum at (a a^2) with f(a a^2) = 0. Its gradient is","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"nabla f_a(symbfx) =\n    beginbmatrix\n        -4b(x_2 - x_1^2)x_1 - 2(a-x_1) \n        2b(x_2 - x_1^2)\n    endbmatrix\n=\n    beginbmatrix\n        4bx_1^3 -4b x_1 x_2 + 2x_1 - 2a\n        \n        -2b x_1^2 + 2b x_2\n    endbmatrix","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Let's plot the Rosenbrock function's contours to see the valley:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"# We use `CairoMakie` for plotting.\nusing CairoMakie\nusing Printf\n\n# Define the function:\nf(x1, x2, a, b) = b * (x2 - x1^2)^2 + (a - x1)^2\n\n# I am using a `let` block here to not pollute the global scope ...\ncolors = Makie.wong_colors()\nlet\n    set_theme!(DOC_THEME2) #hide\n    # evaluation range\n    X1 = LinRange(-2, 2, 100)\n    X2 = X1\n    # define function for ``a=1, b=100``\n    F = (x1, x2) -> f(x1, x2, 1, 100)\n\n    # initialize figure\n    fig = Figure(;)\n\n    # set global title\n    Label(fig[1, 1:4], \"Rosenbrock Function.\"; fontsize=60f0)\n\n    # plot filled contours in left axis\n    ax1 = Axis(fig[2,2]; aspect=1, title=L\"f(x_1, x_2)\", ylabelvisible=false)\n    c = contourf!(ax1, X1, X2, F)\n    scatter!(ax1, (1, 1))\n    # and also give it a colorbar\n    Colorbar(fig[2,1], c;\n        flipaxis=false, ticks=[0, 1e3, 2e3, 3e3],\n        tickformat=nums->[@sprintf(\"%dK\",n/1000) for n in nums]\n    )\n\n    # plot log contours in right axis\n    ax2 = Axis(fig[2,3], aspect=1, title=L\"\\log(f(x_1, x_2)))\")\n    c = contourf!(ax2, X1, X2, log10 âˆ˜ F)\n    scatter!(ax2, (1, 1))\n    Colorbar(fig[2,4], c;\n        ticks=[-2, -1, 0, 1, 2, 3],\n    )\n\n    linkaxes!(ax1, ax2)\n\n    # make colorbars have nice size\n    rowsize!(fig.layout, 2, Aspect(2,1))\n\n    # display plot\n    fig\nend","category":"page"},{"location":"generated/two_rosenbrock/#Bi-Objective-Problem","page":"2 Rosenbrocks","title":"Bi-Objective Problem","text":"","category":"section"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Now let a_1 a_2 in â„ and b_1  0 b_2  0, define f_1 = f_a_1 b_1 and f_2 = f_a_2 b_2, and consider","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"min_symbfxin â„^2\n    beginbmatrix\n        f_1(symbf x)\n        \n        f_2(symbf x)\n    endbmatrix","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"The KKT conditions read","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Î±\n    beginbmatrix\n        4b_1x_1^3 - 4b_1 x_1 x_2 + 2x_1 - 2a_1\n        \n        -2b_1 x_1^2 + 2b_1 x_2\n    endbmatrix\n+ (1-Î±)\n    beginbmatrix\n        4b_2x_1^3 - 4b_2 x_1 x_2 + 2x_1 - 2a_2\n        \n        -2b_2 x_1^2 + 2b_2 x_2\n    endbmatrix\n=0 qquad Î±in 01","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"The second equation gives","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"beginaligned\n-2(Î±b_1 + (1-Î±)b_2)x_1^2 + 2(Î±b_1 + (1-Î±)b_2)x_2 = 0 \n quad x_2 = x_1^2\nendaligned","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"With this, the terms 4b_1x_1^3 - 4b_1 x_1 x_2 and 4b_2x_1^3 - 4b_2 x_1 x_2 cancel out and the first equation becomes:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"beginaligned\n0 = 2x_1 - 2Î±a_1 -2(1-Î±)a_2\n  x_1 = ( Î±a_1 + (1-Î±) a_2)\nendaligned","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Thus, the Pareto-critical set is a parabolic segment with x_1 ranging from a_1 to a_2 and x_2 = x_1^2.","category":"page"},{"location":"generated/two_rosenbrock/#Plotting-the-Pareto-Set","page":"2 Rosenbrocks","title":"Plotting the Pareto-Set","text":"","category":"section"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Let's fix some parameters and define our objectives:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"a1 = 1.0\na2 = 2.0\nb1 = b2 = 100\nf1(x1, x2) = f(x1, x2, a1, b1)\nf2(x1, x2) = f(x1, x2, a2, b2)","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Again, I do the plotting in a let block, because I might want to use other ranges later on:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"let\n    set_theme!(DOC_THEME2) #hide\n\n    # evaluation range\n    X1 = LinRange(-4.1, 4.1, 100)\n    X2 = X1\n\n    # initialize figure\n    fig = Figure(;)\n    ax1 = Axis(fig[2,2]; aspect=1, title=L\"f_1(x_1, x_2)\", ylabelvisible=false)\n    ax2 = Axis(fig[2,3], aspect=1, title=L\"f_2(x_1, x_2)\")\n\n    linkaxes!(ax1, ax2)\n\n    # set global title\n    Label(fig[1, 1:4], \"2 Rosenbrock Functions.\"; fontsize=60f0)\n\n    # plot filled contours in left axis\n    c = contourf!(ax1, X1, X2, f1)\n    # and also give it a colorbar\n    Colorbar(fig[2,1], c;\n        flipaxis=false,\n        tickformat=nums->[@sprintf(\"%.1fK\",n/1000) for n in nums]\n    )\n\n    # plot log contours in right axis\n    c = contourf!(ax2, X1, X2, f2)\n    Colorbar(fig[2,4], c;\n            tickformat=nums->[@sprintf(\"%.1fK\",n/1000) for n in nums]\n    )\n\n    # plot Pareto set into both axes\n    A = LinRange(a1, a2, 100)\n    lines!(ax1, A, A.^2; color=colors[1], label=\"PS\")\n    lines!(ax2, A, A.^2; color=colors[1], label=\"PS\")\n\n    # plot minima\n    scatter!(ax1, (a1, a1^2); color=colors[2])\n    scatter!(ax2, (a2, a2^2); color=colors[3])\n\n    # make colorbars have nice size\n    rowsize!(fig.layout, 2, Aspect(2,1))\n\n    # display plot\n    fig\nend","category":"page"},{"location":"generated/two_rosenbrock/#Optimization","page":"2 Rosenbrocks","title":"Optimization","text":"","category":"section"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"For multi-objective optimization, we use our package:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"import MultiobjectiveNonlinearCG as M","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"The gradients are known analitically:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"df(x1, x2, a, b) = [\n    -4*b*(x2-x1^2)*x1 - 2*(a-x1),\n    2*b*(x2-x1^2)\n]\n\ndf1(x1, x2) = df(x1, x2, a1, b1)\ndf2(x1, x2) = df(x1, x2, a2, b2)","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"We need a vector-vector objective and a vector-matrix transposed jacobian:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"objf(x) = [f1(x[1], x[2]), f2(x[1], x[2])]\njacT(x) = hcat(df1(x[1], x[2]), df2(x[1], x[2]))","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Reproducibly initialize starting site:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"import Random\nRandom.seed!(2178)\nx0 = -4 .+ 8 .+ rand(2)","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Setup an optimization run to gather information in cache","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"function do_experiment(x0; descent_rule, max_iter=100)\n    cache = M.GatheringCallbackCache(Float64)\n    callbacks = vcat(M.DEFAULT_CALLBACKS, M.GatheringCallback(cache))\n    _ = M.optimize(x0, objf, jacT; descent_rule, max_iter, callbacks)\n    return cache\nend","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Compare Steepest Descent and a PRP variant:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"c_sd = do_experiment(x0; descent_rule=M.SteepestDescentRule(M.StandardArmijoRule()))\nc_prp = do_experiment(x0; descent_rule=M.PRP(M.ModifiedArmijoRule(), :sd))","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Stacking iteration sites into matrices makes calculation of axis bounds easier:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"X_sd = reduce(hcat, c_sd.x_arr)\nX_prp = reduce(hcat, c_prp.x_arr)","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"I use a helper for this","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"function get_lims(Xs...; margin=0.1)\n    xlims = [Inf, -Inf]\n    ylims = [Inf, -Inf]\n\n    for X in Xs\n        _xlims, _ylims = extrema(X, dims=2)\n        xlims[1] = min(xlims[1], _xlims[1])\n        xlims[2] = max(xlims[2], _xlims[2])\n        ylims[1] = min(ylims[1], _ylims[1])\n        ylims[2] = max(ylims[2], _ylims[2])\n    end\n\n    if margin > 0\n        xw = xlims[2] - xlims[1]\n        xlims[1] -= margin * xw\n        xlims[2] += margin * xw\n\n        yw = ylims[2] - ylims[1]\n        ylims[1] -= margin * yw\n        ylims[2] += margin * yw\n    end\n\n    return Tuple(xlims), Tuple(ylims)\nend","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"Finally, the plotting is done much the same as before:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"(x1_min, x1_max), (x2_min, x2_max) = get_lims(X_sd, X_prp)\nlet\n    set_theme!(DOC_THEME2) #hide\n    # evaluation range\n    X1 = LinRange(x1_min, x1_max, 100)\n    X2 = LinRange(x2_min, x2_max, 100)\n    # initialize figure\n    fig = Figure(;)\n    ax1 = Axis(fig[2,2]; aspect=1, title=L\"f_1(x_1, x_2)\", ylabelvisible=false)\n    ax2 = Axis(fig[2,3], aspect=1, title=L\"f_2(x_1, x_2)\")\n\n    linkaxes!(ax1, ax2)\n\n    # set global title\n    Label(fig[1, 1:4], \"2 Rosenbrock Functions.\"; fontsize=60f0)\n\n    # plot filled contours in left axis\n    c = contourf!(ax1, X1, X2, f1)\n    # and also give it a colorbar\n    Colorbar(fig[2,1], c;\n        flipaxis=false,\n        tickformat=nums->[@sprintf(\"%.1fK\",n/1000) for n in nums]\n    )\n\n    # plot log contours in right axis\n    c = contourf!(ax2, X1, X2, f2)\n    Colorbar(fig[2,4], c;\n            tickformat=nums->[@sprintf(\"%.1fK\",n/1000) for n in nums]\n    )\n\n    # plot Pareto set into both axes\n    A = LinRange(a1, a2, 100)\n    lines!(ax1, A, A.^2; color=colors[1], label=\"PS\")\n    lines!(ax2, A, A.^2; color=colors[1], label=\"PS\")\n\n    # make colorbars have nice size\n    rowsize!(fig.layout, 2, Aspect(2,1))\n\n    scatterlines!(ax1, X_prp; markersize=10f0, color=colors[5], label=\"prp\")\n    scatterlines!(ax1, X_sd; markersize=10f0, color=colors[4], label=\"sd\")\n    scatterlines!(ax2, X_prp; markersize=10f0, color=colors[5], label=\"prp\")\n    scatterlines!(ax2, X_sd; markersize=10f0, color=colors[4], label=\"sd\")\n\n    # activate legend\n    axislegend(ax1; position=:lb)\n\n    # display plot\n    fig\nend","category":"page"},{"location":"generated/two_rosenbrock/#Criticality-Plot","page":"2 Rosenbrocks","title":"Criticality Plot","text":"","category":"section"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"The behavior of steepest descent becomes more apparent, when looking at the criticality as defined by the norm of the steepest descent direction.","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"function crit_map(x1, x2)\n    Î´ = M.frank_wolfe_multidir_dual((df1(x1,x2), df2(x1,x2)))\n    return sqrt(sum(Î´.^2))\nend","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"The plot reveals a near critical parabola:","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"let\n    set_theme!(DOC_THEME) #hide\n    # evaluation range (more fine-grained here)\n    X1 = LinRange(x1_min, x1_max, 200)\n    X2 = LinRange(x2_min, x2_max, 200)\n\n    fig = Figure()\n    ax = Axis(fig[1,1]; title=L\"\\log(\\Vert\\mathbf{\\delta}\\Vert^2)\")\n\n    c = heatmap!(ax, X1, X2, log10 âˆ˜ crit_map)\n    Colorbar(fig[1,2], c)\n\n    # plot Pareto set into both axes\n    A = LinRange(a1, a2, 100)\n    lines!(ax, A, A.^2; color=colors[1], label=\"PS\", linewidth=5f0)\n\n    scatterlines!(ax, X_prp; markersize=10f0, color=colors[5], label=\"prp\")\n    scatterlines!(ax, X_sd; markersize=10f0, color=colors[6], label=\"sd\")\n\n    axislegend(ax)\n    fig\nend","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"","category":"page"},{"location":"generated/two_rosenbrock/","page":"2 Rosenbrocks","title":"2 Rosenbrocks","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"EditURL = \"https://github.com/manuelbb-upb/MultiobjectiveNonlinearCG/blob/main/src/dir_rules/multidir_frank_wolfe.jl\"","category":"page"},{"location":"generated/multidir_frank_wolfe/#Custom-Frank-Wolfe-Solver...","page":"Frank-Wolfe Solver","title":"Custom Frank-Wolfe Solver...","text":"","category":"section"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"... to compute the multi-objective steepest descent direction cheaply. For unconstrained problems, the direction can be computed by projecting symbf0in â„^n onto the convex hull of the negative objective gradients. This can be done easily with JuMP and a suitable solver (e.g., COSMO).","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"In â€œMulti-Task Learning as Multi-Objective Optimizationâ€ by Sener & Koltun, the authors employ a variant of the Frank-Wolfe-type algorithms defined in â€œRevisiting Frank-Wolfe: Projection-Free Sparse Convex Optimizationâ€ by Jaggi.","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"The objective for the projection problem is","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"F(symbfÎ±)\n= frac12  sum_i=1^K Î±áµ¢ fáµ¢ _2^2\n= frac12  nabla symbff^T symbfÎ± _2^2\n= frac12 symbf a^T nabla symbff nabla symbff^T symbf Î±","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Hence,","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"nabla F(symbfÎ±)\n= nabla symbff nabla symbff^T symbf Î±\n= symbf M symbf Î±","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"The algorithm starts with some initial symbfÎ± = Î±_1  Î±_K^T and optimizes F within the standard simplex S = symbf Î± = Î±_1  Î±_k Î±_i ge 0 sum_i Î±_i = 1 This leads to the following procedure:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Compute seed s as the minimizer of langle symbf s nabla F(symbf Î±_k) rangle = langle symbf s symbf M symbf Î±_krangle in S. The minimum is attained in one of the corners, i.e., symbf s = symbf e_t, where t is the minimizing index for the entries of symbf M symbf Î±_k.\nCompute the exact stepsize Î³in01 that minimizes\nF((1-Î³)symbf Î±_k + Î³ symbf s)\nSet symbf Î±_k+1 = (1-Î³_k) Î±_k + Î³_k symbf s.","category":"page"},{"location":"generated/multidir_frank_wolfe/#Finding-the-Stepsize","page":"Frank-Wolfe Solver","title":"Finding the Stepsize","text":"","category":"section"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Let's discuss step 2. Luckily, we can indeed (easily) compute the minimizing stepsize. Suppose symbf v  â„â¿ and symbf u  â„â¿ are vectors and symbf M  â„^nn is a symmetric square matrix. What is the minimum of the following function?","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Ïƒ(Î³) = ( (1-Î³) symbf v + Î³ symbf u )áµ€ symbf M ( (1-Î³) symbf v + Î³ symbf u) qquadâ€ƒ (Î³  01)","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"We have","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Ïƒ(Î³) beginalignedt\n\t=\n\t( (1-Î³) symbf v + Î³ symbf u )áµ€symbfM ( (1-Î³) symbf v + Î³ symbfu)\n\t\t\n\t=\n\t(1-Î³)Â² underbracesymbfváµ€symbfM symbfv_a +\n\t  2Î³(1-Î³) underbracesymbfuáµ€symbfM symbfv_b +\n\t    Î³Â² underbracesymbfuáµ€symbfM symbfu_c\n\t\t\n\t=\n\t(1 + Î³Â² - 2Î³)a + (2Î³ - 2Î³Â²)b + Î³Â² c\n\t\t\n\t=\n\t(a -2b + c) Î³Â² + 2 (b-a) Î³ + a\nendaligned","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"The variables a b and c are scalar. The boundary values are","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Ïƒâ‚€ = Ïƒ(0) = aâ€ƒtextandâ€ƒÏƒâ‚ = Ïƒ(1) = c","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"If (a-2b+c)  0  a-b  b-c, then the parabola is convex and has its global minimum where the derivative is zero:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"2(a - 2b + c) y^* + 2(b-a) stackrel= 0\nâ€ƒ\n\tÎ³^* = frac-2(b-a)2(a -2 b + c)\n\t\t= fraca-b(a-b)+(c-b)","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"If a-b  b -c, the parabola is concave and this is a maximum. The extremal value is","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Ïƒ_* = Ïƒ(Î³^*)\n\t= frac(a - b)^2(a-b)+(c-b) - frac2(a-b)^2(a-b) + (c-b) + a\n\t= a - frac(a-b)^2(a-b) + (c-b)","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"\"\"\"\n\tmin_quad(a,b,c)\n\nGiven a quadratic function ``(a -2b + c) Î³Â² + 2 (b-a) Î³ + a`` with ``Î³ âˆˆ [0,1]``, return\n`Î³_opt` minimizing the function in that interval and its optimal value `Ïƒ_opt`.\n\"\"\"\nfunction min_quad(a,b,c)\n\ta_min_b = a-b\n\tb_min_c = b-c\n\tif a_min_b > b_min_c\n\t\t# the function is a convex parabola and has its global minimum at `Î³`\n\t\tÎ³ = a_min_b /(a_min_b - b_min_c)\n\t\tif 0 < Î³ < 1\n\t\t\t# if its in the interval, return it\n\t\t\tÏƒ = a - a_min_b * Î³\n\t\t\treturn Î³, Ïƒ\n\t\tend\n\tend\n\t# the function is either a line or a concave parabola, the minimum is attained at the\n\t# boundaries\n\tif a <= c\n\t\treturn 0, a\n\telse\n\t\treturn 1, c\n\tend\nend","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"To use the above function in the Frank-Wolfe algorithm, we define a helper according to the definitions of ab and c:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"function min_chull2(M, v, u)\n\tMv = M*v\n\ta = v'Mv\n\tb = u'Mv\n\tc = u'M*u\n\treturn min_quad(a,b,c)\nend","category":"page"},{"location":"generated/multidir_frank_wolfe/#Completed-Algorithm","page":"Frank-Wolfe Solver","title":"Completed Algorithm","text":"","category":"section"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"The stepsize computation is the most difficult part. Now, we only have to care about stopping and can complete the solver for our sub-problem:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"import LinearAlgebra as LA","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"function frank_wolfe_multidir_dual(grads; max_iter=10_000, eps_abs=1e-6)\n\n\tnum_objfs = length(grads)\n\tT = Base.promote_type(Float32, mapreduce(eltype, promote_type, grads))\n\n\t# 1) Initialize ``Î±`` vector. There are smarter ways to do this...\n\tÎ± = fill(T(1/num_objfs), num_objfs)\n\n\t# 2) Build symmetric matrix of gradient-gradient products\n\t# # `_M` will be a temporary, upper triangular matrix\n\t_M = zeros(T, num_objfs, num_objfs)\n\tfor (i,gi) = enumerate(grads)\n\t\tfor (j, gj) = enumerate(grads)\n\t\t\tj<i && continue\n\t\t\t_M[i,j] = gi'gj\n\t\tend\n\tend\n\t# # mirror `_M` to get the full symmetric matrix\n\tM = LA.Symmetric(_M, :U)\n\n\t# 3) Solver iteration\n\t_Î± = copy(Î±)    \t\t# to keep track of change\n\tu = zeros(T, num_objfs) # seed vector\n\tfor _=1:max_iter\n\t\tt = argmin( M*Î± )\n\t\tv = Î±\n\t\tfill!(u, 0)\n\t\tu[t] = one(T)\n\n\t\tÎ³, _ = min_chull2(M, v, u)\n\n\t\tÎ± .*= (1-Î³)\n\t\tÎ±[t] += Î³\n\n\t\tif sum( abs.( _Î± .- Î± ) ) <= eps_abs\n\t\t\tbreak\n\t\tend\n\t\t_Î± .= Î±\n\tend\n\n\t# return -sum(Î± .* grads) # somehow, broadcasting leads to type instability here,\n\t# see also https://discourse.julialang.org/t/type-stability-issues-when-broadcasting/92715\n\treturn mapreduce(*, +, Î±, grads)\nend","category":"page"},{"location":"generated/multidir_frank_wolfe/#Caching","page":"Frank-Wolfe Solver","title":"Caching","text":"","category":"section"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Looking into frank_wolfe_multidir_dual, we see that in each execution there are allocations. As the function is called repeatedly in some outer loop, it might proof beneficial to pre-allocate these arrays and use a cached version of the algorithm:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"struct FrankWolfeCache{T}\n\tÎ± :: Vector{T}\n\t_Î± :: Vector{T}\n\t_M :: Matrix{T}\n\tu :: Vector{T}\n\tsol :: Vector{T}\nend","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"The initializer works just as in frank_wolfe_multidir_dual:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"function init_frank_wolfe_cache(grads)\n\tnum_objfs = length(grads)\n\tT = Base.promote_type(Float32, mapreduce(eltype, promote_type, grads))\n\treturn init_frank_wolfe_cache(T, num_objfs)\nend\n\nfunction init_frank_wolfe_cache(T, num_vars, num_objfs)\n\t# 1) Initialize ``Î±`` vector. There are smarter ways to do this...\n\tÎ± = fill(T(1/num_objfs), num_objfs)\n\t_Î± = copy(Î±)\n\n\t# 2) Build symmetric matrix of gradient-gradient products\n\t# # `_M` will be a temporary, upper triangular matrix\n\t_M = zeros(T, num_objfs, num_objfs)\n\n\t# seed vector\n\tu = zeros(T, num_objfs)\n\n\t# solution vector\n\tsol = zeros(T, num_vars)\n\treturn FrankWolfeCache(Î±, _Î±, _M, u, sol)\nend","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"Of course, the new method ends in \"!\" to show that it mutates the cache. Also, we return the negative solution here, to avoid unnecessary multiplications later on:","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"function frank_wolfe_multidir_dual!(fw_cache::FrankWolfeCache{T}, grads; max_iter=10_000, eps_abs=1e-6) where T\n\t# Unpack working arrays from cache:\n\tÎ± = fw_cache.Î±\n\t_Î± = fw_cache._Î±\n\t_M = fw_cache._M\n\tu = fw_cache.u\n\tsol = fw_cache.sol\n\n\t# 2) Build symmetric matrix of gradient-gradient products\n\tfor (i,gi) = enumerate(grads)\n\t\tfor (j, gj) = enumerate(grads)\n\t\t\tj<i && continue\n\t\t\t_M[i,j] = gi'gj\n\t\tend\n\tend\n\t# # mirror `_M` to get the full symmetric matrix\n\tM = LA.Symmetric(_M, :U)\n\n\t# 3) Solver iteration\n\t_Î± .= Î±    \t\t\t\t# to keep track of change\n\tfor _=1:max_iter\n\t\tt = argmin( M*Î± )\n\t\tv = Î±\n\t\tfill!(u, 0)\n\t\tu[t] = one(T)\n\n\t\tÎ³, _ = min_chull2(M, v, u)\n\n\t\tÎ± .*= (1-Î³)\n\t\tÎ±[t] += Î³\n\n\t\tif sum( abs.( _Î± .- Î± ) ) <= eps_abs\n\t\t\tbreak\n\t\tend\n\t\t_Î± .= Î±\n\tend\n\n\tfill!(sol, zero(T))\n\tfor (Î±â„“, gâ„“) in zip(Î±, grads)\n\t\tsol .-= Î±â„“ .* gâ„“\n\tend\n\n\treturn nothing\nend","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"","category":"page"},{"location":"generated/multidir_frank_wolfe/","page":"Frank-Wolfe Solver","title":"Frank-Wolfe Solver","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"EditURL = \"https://github.com/manuelbb-upb/MultiobjectiveNonlinearCG/blob/main/docs/src/literate_jl/two_parabolas.jl\"","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"include(joinpath(joinpath(\"..\", \"literate_jl\"), \"makie_theme.jl\")) #hide\nnothing #hide","category":"page"},{"location":"generated/two_parabolas/#Two-Parabolas-Example","page":"2 Parabolas","title":"Two-Parabolas Example","text":"","category":"section"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"The two parabolas problem in 2D reads as","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"min_symbfx  â„^2\n beginbmatrix\n    f_1(symbfx)\n    \n    f_2(symbfx)\n endbmatrix\n =\n beginbmatrix\n (xâ‚ - 1)^2 + (xâ‚‚ - 1)^2\n \n (xâ‚ + 1)^2 + (xâ‚‚ + 1)^2\n endbmatrix\n ","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"The Pareto-Set is the line connecting the individual minima, i.e.,","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"mathcal P_S\n  =\n    left\n      symbfx  â„^2 xâ‚ = xâ‚‚ xâ‚  -1 1\n    right","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"It's easy to setup for MultiobjectiveNonlinearCG. At the lowest level, its optimize method requires a starting point, its image vector, a modifying objective functional and a functional setting the transposed jacobian. Let's define all that:","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"\"Evaluate the objectives at `x` and store result in `y`.\"\nfunction objf!(y, x)\n    y[1] = sum( (x .- 1).^2 )\n    y[2] = sum( (x .+ 1).^2 )\n    return nothing\nend\n\n\"Evaluate objective derivatives at `x` and store transposed jacobian in `DfxT`.\"\nfunction jacT!(DfxT, x)\n    DfxT[:, 1] = x .- 1\n    DfxT[:, 2] = x .+ 1\n    DfxT .*= 2\n    return nothing\nend\n\n# initialize starting values\nimport Random\nx0 = 10 .* [-Ï€, 2*â„¯]\nfx0 = zeros(2)\nobjf!(fx0, x0)","category":"page"},{"location":"generated/two_parabolas/#Optimize-using-Steepest-Descent","page":"2 Parabolas","title":"Optimize using Steepest Descent","text":"","category":"section"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"import MultiobjectiveNonlinearCG as M","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"To use multi-objective steepest descent, we'll first have to decide on a stepsize procedure. We can use a fixed stepsize, standard Armijo or modified Armijo backtracking. For steepest descent, it's most sensible to use standard Armijo:","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"descent_rule = M.SteepestDescentRule(M.StandardArmijoRule())","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"We are nearly ready to go. A maximum number of iterations can be provided by the max_iter keyword argument.","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"max_iter = 100","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"There is also a set of default stopping criteria, which can be inspected by looking at M.DEFAULT_CALLBACKS. To have a fair comparison, we reset them. Additionally, we use a special gathering callback to later plot the iterates:","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"cache1 = M.GatheringCallbackCache(Float64)\ncallbacks = [\n  M.CriticalityStop(; eps_crit=1e-6),\n  M.GatheringCallback(cache1),\n]\n\nx_fin, fx_fin, stop_code, meta1 = M.optimize(\n  x0, objf!, jacT!;\n  objf_is_mutating=true,\n  jacT_is_mutating=true,\n  fx0, max_iter, callbacks, descent_rule,\n)\nmeta1.num_iter[]","category":"page"},{"location":"generated/two_parabolas/#Optimize-with-Modified-PRP-Direction","page":"2 Parabolas","title":"Optimize with Modified PRP Direction","text":"","category":"section"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"First initialize the gathering callback for this trial:","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"cache2 = M.GatheringCallbackCache(Float64)\ncallbacks = [\n  M.CriticalityStop(; eps_crit=1e-6),\n  M.GatheringCallback(cache2),\n]","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"Now, test some non-linear conjugate gradient direction:","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"descent_rule = M.PRP(M.ModifiedArmijoRule(), :sd)\n#descent_rule = M.FRRestart(M.ModifiedArmijoRule(), :sd)\nx_fin, fx_fin, stop_code, meta2 = M.optimize(\n  x0, objf!, jacT!;\n  objf_is_mutating=true,\n  jacT_is_mutating=true,\n  fx0, max_iter, callbacks, descent_rule,\n)\nmeta2.num_iter[]","category":"page"},{"location":"generated/two_parabolas/#Plotting-the-results","page":"2 Parabolas","title":"Plotting the results","text":"","category":"section"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"# We use `CairoMakie` for plotting.\nusing CairoMakie\nusing Printf\nset_theme!(DOC_THEME) #hide\n\n# the `let` block is optional and used just to avoid polluting the global scope\nlet\n  fig = Figure()\n  ax = Axis(fig[1,1]; aspect=1)\n\n  colors = Makie.wong_colors()\n  lines!(ax, [(-1,-1), (1,1)]; linewidth=10f0, label=\"PS\", color=colors[1])\n  scatterlines!(ax, Tuple.(cache1.x_arr), label=\"sd ($(meta1.num_iter))\", color=colors[2])\n  scatterlines!(ax, Tuple.(cache2.x_arr), label=\"prp ($(meta2.num_iter))\", color=colors[3])\n\n  axislegend(ax)\n\n  fig\nend","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"Both runs finish after two iterations :)","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"","category":"page"},{"location":"generated/two_parabolas/","page":"2 Parabolas","title":"2 Parabolas","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#MultiobjectiveNonlinearCG.jl","page":"MultiobjectiveNonlinearCG.jl","title":"MultiobjectiveNonlinearCG.jl","text":"","category":"section"},{"location":"","page":"MultiobjectiveNonlinearCG.jl","title":"MultiobjectiveNonlinearCG.jl","text":"Documentation for MultiobjectiveNonlinearCG.jl","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"EditURL = \"https://github.com/manuelbb-upb/MultiobjectiveNonlinearCG/blob/main/docs/src/literate_jl/alice_bob_plot.jl\"","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"include(joinpath(joinpath(\"..\", \"literate_jl\"), \"makie_theme.jl\")) #hide\nnothing #hide","category":"page"},{"location":"generated/alice_bob_plot/#Pareto-Optimality","page":"Pareto Optimality","title":"Pareto-Optimality","text":"","category":"section"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Consider the formal optimization problem","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"min_symbfxin â„^N\nbeginbmatrix\n    f_1(symbfx)\n    \n    vdots\n    \n    f_K(symbfx)\nendbmatrix\ntagMOP","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"What are solutions of a problem with multiple objectives? Well, acceptable trade-offs between the objective function values. Formally, we use the concept of Pareto-optimality.","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"A point symbfx^*in â„^N is called Pareto-optimal if there is no symbfxin â„^N","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"that is at least as good as symbfx^*, i.e., no symbfx with f_â„“(symbfx)  f_â„“(symbfx^*) for all â„“=1K,\nand that is strictly better in at least one objective, i.e., no symbfx for which there is some â„“in1K with f_â„“(symbfx)f_â„“(symbfx^*).","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"The set of all optimal solutions is the Pareto-Set and its image is the Pareto-Front.","category":"page"},{"location":"generated/alice_bob_plot/#Example","page":"Pareto Optimality","title":"Example","text":"","category":"section"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Alice ğŸ‘© and Bob ğŸ‘¨ want to meet. Alice lives at (1 1) and Bob lives at (-1 -1). So the distance to Alice's home is","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"d_A(x_1 x_2) = sqrt(x_1-1)^2 + (x_2-1)^2","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"For Bob, it is","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"d_B(x_1 x_2) = sqrt(x_1+1)^2 + (x_2+1)^2","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Deciding on a meeting venue is the bi-objective optimization problem","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"min_symbfxin â„2\nbeginbmatrix\n    d_A(symbf x)\n    \n    d_B(symbf x)\nendbmatrix","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Let's visualize the situation:","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"using CairoMakie\nusing FileIO","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Define the distance functions:","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"dA(x1, x2) = sqrt((x1-1)^2 + (x2-1)^2)\ndB(x1, x2) = sqrt((x1+1)^2 + (x2+1)^2)","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"A function to give us a basic figure with Alice and Bob in it:","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"markerA = load(joinpath(joinpath(\"..\", \"literate_jl\"),\"woman.png\"))\nmarkerB = load(joinpath(joinpath(\"..\", \"literate_jl\"),\"man.png\"))\n\nfunction setup_fig()\n    global markerA, markerB\n    set_theme!(DOC_THEME2) #hide\n    fig = Figure()\n\n    # set a title\n    Label(fig[1,1:2], \"Alice and Bob Want to Meet.\")\n\n    # left axis - decision space, where Alice and Bob live\n    ax1 = Axis(fig[2,1]; aspect=1.0)\n    xlims!(ax1, (-1.5, 1.5))\n    ylims!(ax1, (-1.5, 1.5))\n\n    # set markers for Alice and Bob\n    scatter!(ax1, (1, 1); marker=markerA, markersize=30)\n    scatter!(ax1, (-1, -1); marker=markerB, markersize=30)\n\n    # right axis - objective space, distance values\n    ax2 = Axis(fig[2,2]; aspect=1.0, xlabel=L\"d_A\", ylabel=L\"d_B\")\n    xlims!(ax2, (-0.1, 3.5))\n    ylims!(ax2, (-0.1, 3.5))\n    return fig, ax1, ax2\nend\n\n# show the basic image:\nfirst(setup_fig())","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"To compare various meeting positions, we build a function testing for Pareto-optimality in a discrete set with value vectors Y.","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"function check_optimal(Y)\n    _Y = empty(Y)               # array of processed value vectors\n    isoptimal = zeros(Bool, 0)  # flags\n    for fi in Y\n        fi_isoptim = true\n        for (j,fj) in enumerate(_Y)\n            if all(fj .<= fi) && any(fj .< fi)\n                # fj is better than fi\n                fi_isoptim = false\n                break\n            end\n            # test, if fi is better than fj\n            if isoptimal[j]\n                if all(fi .<= fj) && any( fi .< fj)\n                    isoptimal[j] = false\n                end\n            end\n        end\n        push!(_Y, fi)\n        isoptimal = vcat(isoptimal, fi_isoptim)\n    end\n    return isoptimal\nend","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Let's look at some meeting places. First, only consider Alices' home:","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"X = [(1.0, 1.0),]\nY = [(dA(x[1], x[2]), dB(x[1], x[2])) for x in X]","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Obviously, the point is optimal in our discrete sample:","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"isopt = check_optimal(Y)\nall(isopt) == true","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"With more points, it gets more complicated. We want to scatter the X and Y points and use different markers for optimal and non-optimal points.","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"function scatterOpt!(axX, axY, X, Y, isopt; colors)\n    @assert length(X) == length(Y) == length(isopt)\n\n    # plot non-optimal points first\n    si = sortperm(isopt)\n\n    for i=si\n        color = colors[i]\n        marker, strokewidth, strokecolor = if isopt[i]\n            (:circle, 0.5, :black)\n        else\n            (:xcross, 1.5, :red)\n        end\n\n        scatter!(axX, (X[i]...); color, marker, strokewidth, strokecolor)\n        scatter!(axY, (Y[i]...); color, marker, strokewidth, strokecolor)\n    end\n    nothing\nend","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Now, Alice's place should get a round marker.","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"let\n    fig, ax1, ax2 = setup_fig()\n    scatterOpt!(ax1, ax2, X, Y, isopt; colors=Makie.wong_colors())\n    fig\nend","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Now we include Bob's home.","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"X = [\n    (1.0, 1.0),\n    (-1.0, -1.0)\n]\nY = [(dA(x[1], x[2]), dB(x[1], x[2])) for x in X]","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Still, both points are Pareto-optimal in our sample set.","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"isopt = check_optimal(Y)\nisopt == [true, true]","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"This is reflected in the plot. Note, that individual minima are by definition always Pareto-optimal, also in the non-discrete case.","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"let\n    fig, ax1, ax2 = setup_fig()\n    scatterOpt!(ax1, ax2, X, Y, isopt; colors=Makie.wong_colors())\n    fig\nend","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"By adding a third point, we see that it is important to distinguish our discrete comparisons and the continuous case.","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"X = [\n    (1.0, 1.0),\n    (-1.0, -1.0),\n    (-1.0, 1.0),\n]\nY = [(dA(x[1], x[2]), dB(x[1], x[2])) for x in X]\nlet\n    fig, ax1, ax2 = setup_fig()\n    scatterOpt!(ax1, ax2, X, Y, check_optimal(Y); colors=Makie.wong_colors())\n    fig\nend","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Surprisingly, (-1 1) is considered optimal, despite it being fare away from Alice and Bob. (00) would certainly be preferrable for both:","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"X = [\n    (1.0, 1.0),\n    (-1.0, -1.0),\n    (-1.0, 1.0),\n    (0.0, 0.0),\n]\nY = [(dA(x[1], x[2]), dB(x[1], x[2])) for x in X]\n\nfig, ax1, ax2 = setup_fig()\nscatterOpt!(ax1, ax2, X, Y, check_optimal(Y); colors=Makie.wong_colors())\nfig","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"That's it! With more samples we get a better sense for the optimal points of the continuous problem (MOP). Thus, let's use a pseudo-random sampling of decision space and see if we can identify the Pareto Set.","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"using HaltonSequences\nusing ColorSchemes\nnum_X = 200\nX = [Tuple( -1.4 .+ 2.8 .* p  ) for p in HaltonPoint(2; length=num_X)]\nY = [(dA(x[1], x[2]), dB(x[1], x[2])) for x in X]\nscatterOpt!(\n    ax1, ax2, X, Y, check_optimal(Y);\n    colors=[get(ColorSchemes.acton, (i-1)/(num_X-1)) for i=1:num_X]\n)\nfig","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"Ok, we got a rough sense for the Pareto Set. Indeed, we can analitically show that its the line from (-1 -1) to (11).","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"lines!(ax1, [(-1, -1), (1, 1)]; linewidth=5f0, color=:blue)\nPF = [(dA(x,x), dB(x, x)) for x=1:-0.1:-1]\nlines!(ax2, PF; linewidth=5f0, color=:blue)\nfig","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"","category":"page"},{"location":"generated/alice_bob_plot/","page":"Pareto Optimality","title":"Pareto Optimality","text":"This page was generated using Literate.jl.","category":"page"}]
}
